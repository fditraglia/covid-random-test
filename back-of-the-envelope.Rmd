---
title: "A Short Note on Surveillance Testing"
author: "FD, CG, & VK on behalf of LMH Governing Body"
date: "13/09/2020"
fontsize: 12pt
output: pdf_document
urlcolor: blue
fig_caption: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This document outlines some simple strategies to assess the prevalence of coronavirus when there is insufficient capacity to test everyone in the population of interest. The problem of trying to determine whether a given individual has coronavirus is fundamentally different from the problem of estimating the prevalence of infection. The high test specificity and sensitivity along with rapid turnaround that are crucial for individual medical diagnosis are not absolutely essential for surveillance testing. The purpose of this document is to suggest that relatively modest resources may be sufficient to obtain valuable information regarding the spread and prevalence of coronavirus in the Oxford population. We view this issue as separate from and parallel to the the "Early Alert Testing" service. The ideas we discuss are neither novel nor original, and we do not propose a detailed plan for surveillance testing. Our goals are merely to explain randomized and pooled testing strategies to a non-specialist audience, and to  calculate the approximate number of tests required to provide useful information. For simplicity we begin by abstracting away the problem of false negatives and false positives. The final section discusses how these affect our analysis.

## The Bottom Line
```{r, echo=FALSE}
get_pool_fraction <- function(pool_size, prob_positive) {
  1 / pool_size + 1 - (1 - prob_positive)^pool_size
}
get_pool_tests <- function(n, pool_size, prob_positive) {
  round(n * get_pool_fraction(pool_size, prob_positive))
}
```
Testing a random sample of 1400 students each week (200 per day) would provide useful estimates of the prevalence and spread of coronavirus at Oxford, allowing us to evaluate the effectiveness of the university's [COVID-19 Student Responsibility Agreement](https://www.ox.ac.uk/coronavirus/students/agreement) and guide potential policy changes.
Using a simple pooling strategy, this would require approximately `r get_pool_tests(1400, 7, 0.0025)` laboratory tests per week (`r get_pool_tests(200, 7, 0.0025)` per day). 
<!--Randomized surveillance testing remains useful even in the presence of false negatives and positives.-->
Because capacity cannot be diverted from the Early Alert Testing service, the university would need to obtain a total of roughly `r round(get_pool_tests(1400 * 8, 7, 0.0025), digits = -3)` laboratory tests from another source over the course of MT. These need not have a rapid turnaround time: a delay of several days would still allow us to assess the spread of infection. Pooled testing reduces the number of laboratory tests required but does not affect the number of samples that need to be collected. To reduce the burden of collecting 200 samples each day, individuals could be asked to self-swab while being supervised by a healthcare professional.[^1] 

[^1]: This is the procedure used by the ONS in their [Coronavirus Infection Survey pilot](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/coronaviruscovid19infectionsurveypilot/11september2020). A recent study suggests that this procedure is no less accurate than having healthcare workers conduct the swabs directly: <https://www.medrxiv.org/content/10.1101/2020.04.11.20062372v1>.


<!--With 1400 tests a week, i.e. 200 tests per day, one could perform randomized testing that gives useful estimates for prevalence of coronavirus in the Oxford student population. With 4000 tests a week, universal testing of the student population with pooling could be carried out. For universal testing to be effective, turnaround times would have to be 24-48 hours at the most, whereas randomized testing would still be useful, albeit with delayed signal, with longer turnaround times.-->

## Randomized Testing

As a rough approximation, suppose that there are 20,000 undergraduate and taught masters students in residence at Oxford during MT. An unknown number $C$ of them have the coronavirus. With a perfect test, we could learn $C$ by testing all 20,000 students. But what if we only have the capacity to carry out $n$ tests, where $n$ is much smaller than 20,000? In this case we cannot learn $C$ exactly, but by employing *randomized testing* we can produce an estimate that is sufficiently accurate to guide our mitigation policies. The value of randomized testing is widely acknowledged. It underlies both the [Coronavirus Infection Survey pilot](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/bulletins/coronaviruscovid19infectionsurveypilot/11september2020) conducted by the ONS, and the [REACT-1](https://www.gov.uk/government/publications/react-1-study-of-coronavirus-transmission-august-2020-results/react-1-real-time-assessment-of-community-transmission-of-coronavirus-covid-19-in-august-2020) survey conducted by the Department of Health and Social Care. Both of these surveys are extremely valuable, but as they aim to be nationally representative, neither employs a sufficiently large sample size to track local trends. As such we propose that randomized surveillance testing be carried out within the university.

The most basic form of randomized testing relies on a *simple random sample*. To use this procedure, we place all 20,000 student id numbers into a hat, mix them up thoroughly, and then draw out $n$ at random. These $n$ students are sent for a coronavirus test. We then use the number of students in our sample who have coronavirus to calculate an estimate $E$ of the number students in the population who have coronavirus. For example, if one student in a sample of 2000 tests positive, then we would estimate that there are there are $10$ cases in the University as a whole. 
<!--Expressed as a formula, if $P$ is the number of students in our sample who test positive, then our estimate $E$ of the total number of cases in the university equals $P/n$ multiplied by 20,000. In the example this equals 10.-->

Because students are chosen at random, the estimated number of cases $E$ may not equal the true number of cases $C$. Purely by chance, we might draw a sample of students in which no one is infected. In this case $E$ would equal zero: an underestimate of $C$. We could also be unlucky in the opposite direction and, purely by chance, draw a sample that contains everyone at Oxford who is infected. In this case $E$ would be an overestimate of $C$. Both of these possibilities, however, are remote: $E$ is highly likely to be close to $C$. With an appropriate sample size, an estimate based on random testing is reliable enough to tell us with high confidence whether the number of cases is, say, greater than 50 or fewer than 20. It can also be used to determine whether prevalence is increasing and, if so, how quickly. 

Relative to the Early Alert Testing system, which tests only symptomatic individuals, a crucial advantage of randomized testing is that it will also detect pre-symptomatic and asymptomatic cases. This is important because, as explained in the  [SAGE report of 3rd September](https://www.gov.uk/government/publications/principles-for-managing-sars-cov-2-transmission-associated-with-higher-education-3-september-2020), "asymptomatic transmission is a key risk in university settings." Indeed, a recent study cited in the SAGE report finds that only 18\% of those aged 0--19 and 22\% of those aged 20--39 who are infected with coronavirus show symptoms of the disease.[^2] Even if these figures are substantial overestimates, it is likely that a considerable fraction of infections among our students will go undetected by the Early Alert Testing system. For this reason, even if the turn-around times for randomized tests were somewhat slower, they could still provide more timely information about the spread of infection. 

[^2]: [https://arxiv.org/abs/2006.08471](https://arxiv.org/abs/2006.08471)

The key question in designing a randomized testing protocol is how large a sample size to use. Fortunately, there are several simple ways to provide an approximate answer. One is to calculate how many students we would need to sample *on average* before we detect our first positive result. If there are $C$ total cases out of 20,000 students in the university then the answer is approximately 20,000$/C$.[^3] The following table presents results for a range of values for $C$. As we see from the table, the more prevalent the infection, the fewer tests we need before we detect a positive. For purposes of comparison, we can say with high confidence based on recent results from [REACT-1](https://www.gov.uk/government/publications/react-1-study-of-coronavirus-transmission-august-2020-results/react-1-real-time-assessment-of-community-transmission-of-coronavirus-covid-19-in-august-2020) that between 0.16\% and 0.41\% of individuals in the 18-24 age group had the coronavirus at the start of September: roughly 30-80 out of 20,000.

[^3]: The exact value is $(N+1)/(C + 1)$ for a population of size $N$ containing $C$ positives.


```{r,echo=FALSE}
N <- 20000
cases <- (2:10) * 10 
prevalence <- 100 * cases / N
n_tests <- round((20000 + 1) / (cases + 1))
foo <- as.data.frame(cbind(cases, prevalence, n_tests))
knitr::kable(foo, col.names = c('#Cases ($C$)', 'Prevalence (%)', 'Expected #Samples'),
             caption = 'Expected number of samples before the first positive result under random sampling without replacement from a population of size 20,000.')
```

```{r,echo=FALSE}
get_est_range <- function(sample_size, Npositive, popn_size = 20000, alpha = 0.2) {
  Plower <- qhyper(alpha / 2, Npositive, popn_size - Npositive, sample_size)
  Pupper <- qhyper(1 - alpha / 2, Npositive, popn_size - Npositive, sample_size)
  Elower <- popn_size * Plower / sample_size
  Eupper <- popn_size * Pupper / sample_size
  return(c(floor(Elower), ceiling(Eupper)))
}
plot_est_range <- function(sample_size, Npositive, popn_size = 20000, alpha = 0.2) {
# vectorized over sample_size
  Plower <- qhyper(alpha / 2, Npositive, popn_size - Npositive, sample_size)
  Pupper <- qhyper(1 - alpha / 2, Npositive, popn_size - Npositive, sample_size)
  Elower <- popn_size * Plower / sample_size
  Eupper <- popn_size * Pupper / sample_size
  matplot(sample_size, cbind(Elower, Eupper),
          xlab = 'Sample Size', ylab = 'Estimated #Cases',
          main = '', pch = '')
  arrows(sample_size, Elower, sample_size, Eupper, length = 0.05,
         angle = 90, code = 3, lwd = 2)
  abline(h = Npositive, lty = 2, col = 'blue', lwd = 2)
  title1 <- 'True #Cases = '
  title2 <- bquote(.(Npositive))
  mytitle <- paste0(title1, title2)
  legend('topright', legend = mytitle)
}
```

A second way to address the question of how many samples are needed is by calculating *how close* the estimated number of cases $E$ will likely be to the true number of cases $C$ for different values of $n$. Suppose that there are 60 cases within the university. This would represent a slight uptick from the most recent  [REACT-1](https://www.gov.uk/government/publications/react-1-study-of-coronavirus-transmission-august-2020-results/react-1-real-time-assessment-of-community-transmission-of-coronavirus-covid-19-in-august-2020) estimate of coronavirus prevalence among 18-24 year olds. With a sample size of $n =1000$ the estimated number of cases $E$ will fall in the range (`r get_est_range(1000, 60)`) with more than 80\% probability. With a sample size of $n = 1500$ this range would narrow to (`r get_est_range(1500, 60)`). The figure overleaf presents analogous results for sample sizes of $500$ and $2000$.[^4]

[^4]: These intervals are calculated from the quantiles of a Hypergeometric($C$, $N - C$, $n$) distribution where $n$ is the sample size and $C$ is the number of positives in a population of size $N$.

```{r, echo = FALSE, fig.width=5, fig.height=4, fig.align = 'center', fig.cap = 'This figure shows how close the estimated number of cases will likely be to the true number of cases for different sample sizes. At a given sample size, the estimated number of cases as a greater than 80\\% chance of falling in the specified interval.'}
plot_est_range(c(500, 1000, 1500, 2000), 60)
```

```{r, echo = FALSE}
get_power <- function(n, p, p0, N = 20000, alpha = 0.1) {
  finite_correct <- (N - n) / (N - 1) # finite popn correction
  mu <- (p - p0) / sqrt(finite_correct * p0 * (1 - p0) /n)
  s <- sqrt(p * (1 - p) / (p0 * (1 - p0)))
  alpha <- 0.1
  critical_value <- qnorm(0.9)
  return(1 - pnorm(critical_value, mu, s))
}
```
A third way of deciding how many samples are needed is by calculating the chance that we will be able to reliably distinguish between two different levels of coronavirus prevalence given a particular sample size. Suppose, for example, that we initially believed there to be $50$ cases in the university, corresponding to the current estimated prevalence among the university-aged population from   [REACT-1](https://www.gov.uk/government/publications/react-1-study-of-coronavirus-transmission-august-2020-results/react-1-real-time-assessment-of-community-transmission-of-coronavirus-covid-19-in-august-2020). If cases increased to 100, we would have approximately a `r round(100 * get_power(1400, p = 100/20000, p0 = 50/20000))`\% chance of detecting this, with high confidence, based on a sample 1400 students.[^5] If they increased to 200, we would have a `r round(100 * get_power(1400, p = 200/20000, p0 = 50/20000))`\% chance of detecting it. Intuitively, if cases were doubling weekly, then a sample of roughly 200 students per day would be likely to detect an increase after one week, and nearly certain to do so after two weeks.

[^5]: These values correspond to a power calculation for an $\alpha = 0.1$ test of $H_0\colon p \leq p_0$ versus $H_1\colon p > p_0$ using the score test statistic $T_n = (\widehat{p} - p_0)/ SE(p_0)$ where $SE(p_0) = \sqrt{\kappa p_0 (1 - p_0)/n}$ and $\kappa = (N - n)/(N - 1)$ is a finite-population correction.

Each of these three calculations shows that a simple random sample of approximately 200 students per day (1400 per week) would already be useful for answering basic population-level questions concerning prevalence and spread. This would amount to around half as many tests over the entire term as would be required for a *single* round of universal testing. The number of tests required could also be dramatically reduced by using a pooled testing strategy, as outlined below.

While a simple random sampling procedure is the easiest to explain and to use for back-of-the-envelope calculations, there are several ways that it could be refined to increase the accuracy of our estimates. It is likely that coronavirus cases will tend to "cluster" within colleges, since the infection is most easily spread through frequent, close contact. If this is so, then a sampling procedure that allocates a *fixed* number of tests to each college (proportional to its size) and draws a simple random sample *within* each college will be more efficient than a simple random sample taken at the university level. This is called a *stratified* random sample. Another possibility is to employ *adaptive sampling*, in which additional tests are allocated to colleges in which positive results are detected in a first batch of tests. While the analysis of such a design is more complicated mathematically, it makes more efficient use of a limited number of tests. In a similar vein, we could design a sampling procedure that exploits the fact that transmission is most likely to occur within household groups.

## Pooled Testing
Pooled testing is a procedure that can be helpful in situations where laboratory capacity, rather than the ability to collect test samples, is the limiting factor in expanding testing. In this procedure, a single laboratory test is carried out on the *combined* samples of a group of individuals, for example a "household" group of 6-8 students. For a sufficiently sensitive test, a negative result for the pooled sample indicates that no one in the group is infected. A positive test indicates that at least one individual in the group is infected, and it is then necessary to test the individual samples to pinpoint the infected individual or individuals.[^6]  When the infection is sufficiently rare, pooled testing is highly efficient because virtually all pooled samples will test negative. For example, if $C = 50$ then pooling tests by household would require approximately `r get_pool_tests(20000, 7, 50/20000)` laboratory tests to achieve the equivalent of universal testing.[^7] 
This strategy has in fact been adopted by the University of Cambridge. In the absence of sufficient laboratory capacity, pooled testing could be combined with randomized testing. For example, our proposal to test a random sample of 1400 students each week would require `r get_pool_tests(1400, 7, 50/20000)` approximately laboratory tests each week. More sophisticated pooling strategies could potentially yield further improvement.[^8]

[^6]: If one's goal is to identify and isolate infected households rather than to estimate the prevalence of coronavirus in the population, there is no need for re-testing. In this case, pooling by household requires only as many tests as there are households, regardless of disease prevalence.

[^7]: The calculations in this paragraph are carried out under the assumptions of [Dorfman (1946)](https://projecteuclid.org/euclid.aoms/1177731363). Under his assumptions, if $k$ is the batch size, $n$ is the total number of samples, and $p$ is the probability that an individual sample tests positive, the expected number of tests required to identify all positive samples equals $n[1/k + 1 - (1 - p)^k]$. In the present context, the assumptions that underlie this calculation are likely to be somewhat pessimistic as they assume independence between the samples within a given batch. In contrast, when pooling by household, we would expect positive dependence between samples in a given batch.

[^8]: See, e.g., [Shental et al. (2020)](https://advances.sciencemag.org/content/6/37/eabc5961?intcmp=trendmd-adv).


## What about false positives and negatives?
Thus far we have abstracted away the problem of false positives and false negatives. As our primary goal is to estimate prevalence and spread rather than to diagnose particular individuals, this is not a major concern. First, if false positive and false negative rates are constant, we can still infer the spread of the infection by examining *changes* in estimated prevalence. Second, recent research has proposed estimates of these rates that can be used to correct estimates of population prevalence. More broadly, as we show in the appendix, false positives are less of a problem among the university-aged population because this group is substantially more likely to be infected.

```{r,echo=FALSE}
get_false_positive <- function(prev, sens, spec) {
# everything is in %
  term1 <- (100 - spec) / sens
  term2 <- (100 - prev) / prev
  return(100 - 100 / (1 + term1 * term2))
}
```

## Appendix: Calculating False Positive Rates

The likelihood of a false positive depends on three factors: the *sensitivity* and *specificity* of a test along with the prevalence of coronavirus in the relevant population.[^9] Sensitivity is the share of people who test *positive* among those who *have coronavirus*. Estimates of the sensitivity of PCR tests vary, but range between 71\% and 98\%.[^10] Specificity is the share of people who test *negative* among those who *do not have coronavirus*. We can bound the sensitivity of PCR tests fairly accurately using results from the [ONS Infection survey pilot](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/methodologies/covid19infectionsurveypilotmethodsandfurtherinformation). Between 1 June and 12 July, 50 of the 122,776 samples collected as part of this survey tested positive. If all 50 of these positive results were false positives, that would imply a specificity of 99.96\%. Accordingly, the true specificity is likely greater than 99.96\%. To be pessimistic, suppose that the sensitivity is 71\% and the specificity is 99.96\%. According to  [REACT-1](https://www.gov.uk/government/publications/react-1-study-of-coronavirus-transmission-august-2020-results/react-1-real-time-assessment-of-community-transmission-of-coronavirus-covid-19-in-august-2020), the prevalence of coronavirus in the population as a whole was approximately 0.13\% at the beginning of September. At this rate of prevalence, no more than `r round(get_false_positive(0.13, 71, 99.96))`\% of positive test results will be false positives. Among university-aged individuals, however, estimated prevalence was approximately 0.25\%. Thus, among students no more than `r round(get_false_positive(0.25, 71, 99.96))`\% of positives will be false positives. For purposes of comparison, coronavirus prevalence among those showing symptoms of the disease was approximately 0.68\% at the beginning of September, according to REACT-1. Thus, under the present government policy of testing only symptomatic individuals, no more than `r round(get_false_positive(0.68, 71, 99.96))`\% positive tests will be false positives. Roughly speaking: false positives are around half as likely among students as in the general population, and half again as likely among symptomatic individuals.

[^9]: Specifically, defining $R = (100\% - \text{Specificity})\times (100\% - \text{Prevalence}) / (\text{Sensitivity} \times \text{Prevalence})$, the false positive rate equals $[R/(1 + R)]$\%.
[^10]: <https://www.bmj.com/content/369/bmj.m1808>






