---
title: "Some Back-of-the-envelope Calculations for Surveillance Testing"
author: "Anonymouse"
date: "11/09/2020"
fontsize: 12pt
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## About This Document

This document outlines some simple strategies to assess the prevalence of coronavirus when there is insufficient capacity to test everyone in the population of interest. The problem of trying to determine whether a given individual has coronavirus is fundamentally different from the problem of estimating the prevalence of coronavirus in a population of interest. The high specificity and sensitivity along with rapid turnaround that are crucial for individual medical diagnosis are not absolutely essential for surveillance testing at the population level. The purpose of this document is to suggest that only relatively modest resources may be sufficient to obtain valuable information regarding the spread and prevalence of coronavirus in the Oxford population. We view this issue as separate from and parallel to the the "Early Alert Testing" service, whose primary purpose is medical. The ideas we discuss are neither novel nor original, and this document does not propose a detailed plan for surveillance testing. Our goals are merely to explain the intuition behind randomized testing and pooled testing for a non-specialist audience, and to give a rough approximation of the number of tests required to provide useful information. To keep the discussion as simple as possible, we begin by abstracting away the problem of false negatives and false positives. The final section discusses how these affect our analysis.

## The Bottom Line
Say something like $x$ number of tests per week could reasonably work if prevalence is such-and-such (with and without pooling)

## Random Testing
As a rough approximation, suppose that there are 20,000 undergraduate and taught masters students in residence at Oxford during MT. An unknown number $C$ of them have the coronavirus. With a perfect test, we could learn $C$ exactly by conducting 20,000 tests. But what if we only have the capacity to carry out $n$ tests, where $n$ is much smaller than 20,000? In this case we cannot learn $C$ exactly, but by employing *random testing* we can produce an estimate that is sufficiently accurate to guide our mitigation policies.

The most basic form of random testing relies on a *simple random sample*. To use this procedure, we place all 20,000 student id numbers into a hat, mix them up thoroughly, and then draw out $n$ at random. These $n$ students are sent for a coronavirus test. We then use the number of students in our *sample* who have coronavirus to estimate the number of students in the *population* who have coronavirus. For example, if one student in a sample of 2000 tests positive, then we would estimate that there are there are $10$ cases in the University as a whole. Expressed as a formula, if $P$ is the number of students in our sample who test positive, then our estimate $E$ of the total number of cases in the university equals $P/n$ multiplied by 20,000. In the example this equals 10.

Because students are chosen at random, the estimated number of cases $E$ may not equal the true number of cases $C$. Purely by chance, we might draw a sample of students in which no one is infected. In this case $E$ would equal zero: an underestimate of $C$. We could also be unlucky in the opposite direction and, purely by chance, draw a sample that contains *everyone* at Oxford who is infected. In this case $E$ would be an overestimate of $C$. While an estimate based on a random sample cannot provide us with perfectly accurate information about the true number of cases, with an appropriate sample size it can tell us with high confidence, say, whether the number of cases is larger than 50 or smaller than 20. Furthermore, it can be used to determine whether the prevalence is increasing and, if so, how quickly. 

A crucial advantage of random testing over the Early Alert Testing system, which tests only symptomatic individuals, is that random testing it will also detect both pre-symptomatic and asymptomatic cases. The SAGE report of 3rd September refers to a study suggesting that, among undergraduate-aged individuals, asymptomatic cases may constitute up to 80\% of total cases. For this reason, even if their turn-around time were somewhat slower, randomized tests could still provide more timely information about the spread of infection.

The key question in designing a random testing protocol is how large a sample size to use. If there are $C$ total cases out of 20,000 students in the university, then on average we would need to carry out approximately 20,000$/C$ tests to detect a positive case.[^1] 
The more prevalent the infection, the fewer tests we need before we detect a positive.
```{r,echo=FALSE}
N <- 20000
cases <- c(10, 20, 50, 100, 200)
prevalence <- 100 * cases / N
n_tests <- round((20000 + 1) / (cases + 1))
foo <- as.data.frame(cbind(cases, prevalence, n_tests))
knitr::kable(foo, col.names = c('#Cases', 'Prevalence (%)', 'Expected #Tests'),
             caption = 'Expected number of tests to detect one positive case in a population of 20,000.')
```
For example, suppose that we can carry out 200 random tests per day. Then within five days we can with high confidence distinguish between a situation in which there are 100 cases in the university as opposed to 20. (FOOTNOTE.)

As another example, suppose that we initially believed that $C = 20$ and suspected that cases might be doubling every week. With 200 tests per day, we would have a roughly even chance of detecting this, with high confidence, in one week, and be almost certain to detect it within two. (FOOTNOTE)

Approximately 200 test per day would already be useful for answering basic population-level questions concerning prevalence and spread. This would amount to around half as many tests over the entire term as a *single* round of universal testing. 

A simple random sampling procedure is the easiest to explain and to use for back-of-the-envelope calculations, but there are several ways that it could be refined to increase the chance that $\widehat{C}$ will be close to $C$ for a given sample size and true number of cases. The first would be to use a *stratified* rather than simple random sample. It is likely that coronavirus cases will tend to "cluster" within colleges, since the infection is most easily spread through frequent, close contact. If this is so, then a sampling procedure that allocates a *fixed* number of tests to each college (proportional to its size) and draws a simple random sample *within* each college will be more efficient than a simple random sample taken at the university level. Another possibility is to employ *adaptive sampling*, in which additional tests are allocated to colleges in which positive tests are detected in a "first wave" of tests. While the analysis of such a design is more complicated mathematically, it makes more efficient use of a limited number of tests if cases tend to cluster within colleges. 


```{r, echo = FALSE}
get_power <- function(n, p, p0, alpha = 0.1) {
  mu <- (p - p0) / sqrt(p0 * (1 - p0) /n)
  s <- sqrt(p * (1 - p) / (p0 * (1 - p0)))
  alpha <- 0.1
  critical_value <- qnorm(0.9)
  return(1 - pnorm(critical_value, mu, s))
}
get_power(1000, p = 100/20000, p0 = 20/20000)
get_power(1400, p = 40/20000, p0 = 20/20000)
get_power(1400, p = 80/20000, p0 = 20/20000)
get_power(1400, p = 160/20000, p0 = 20/20000)
```

This latter point is best understood using an example. Suppose that there are 40 coronavirus cases among 20,000 students. This amounts to a prevalence of $0.2\%$, which would be equivalent to two doublings of the ONS estimate of $0.05\%$ from late August. Accounting for the fact that cases are currently more common among younger age groups, this may be a reasonable approximation to the state of play near the beginning of MT. The following figure shows the different values that the estimated number of cases $\widehat{C}$ could take on, along with their probabilities. In statistical parlance, this is called the "sampling distribution" of $\widehat{C}$. We can calculate these probabilities *exactly* because the randomness in $\widehat{C}$ is entirely under our control: it comes from the fact that students are randomly chosen to be tested.

```{r, echo=FALSE, fig.height = 4}
plot_C_hat <- function(n, C, N = 20000) {
  lower <- qhyper(0.001, C, N - C, n)
  upper <- qhyper(0.999, C, N - C, n)
  C_hat <- N * lower:upper / n
  prob_C_hat <- dhyper(lower:upper, C, N - C, n)
  title1 <- paste('True Cases: ', bquote(.(C)))
  title2 <- paste('Sample Size: ', bquote(.(n)))
  mytitle <- paste0(title1, ', ', title2)
  plot(C_hat, prob_C_hat, type = 'h', ylab = 'Probability', xlab = 'Estimated Cases',
       col = 'blue', lwd = 2, main = mytitle)
}
plot_C_hat(1000, 40)
```

```{r}
d_C_hat <- function(x, n, C, N = 20000) {
  dhyper(n * x / N, C, N - C, n)
}
```



From the figure, we see that $\widehat{C}$ there is a `r round(100 * d_C_hat(40, 1000, 40))`\% chance that our estimate $\widehat{C}$ will equal 40, the true number of cases. Moreover, there is a `r round(100 * sum(d_C_hat(c(20, 40, 60), 1000, 40)))`\% chance that it will be between 20 and 60. There is, of course a chance that our sample will not contain any positives: $\widehat{C}$ equals zero with `r round(100 * d_C_hat(0, 1000, 40))`\% probability.

For a given number of true cases in the population, increasing the sample size makes it more likely that $\widehat{C}$ will be close to $C$. For $n = 2000$ the sampling distribution of $\widehat{C}$ becomes more "bell-shaped." With this increased sample size, there is now a `r round(100 * sum(d_C_hat(10 * (2:6), 2000, 40)))`\% chance that it will be between 20 and 60. The chance that we will fail to detect any cases falls to `r round(100 * d_C_hat(0, 2000, 40))`\%.

```{r, echo = FALSE, fig.height = 4}
plot_C_hat(2000, 40)
```

For a fixed sample size, the probability of failing to detect any cases also falls as the number of true cases rises. Suppose, for example, that the number of cases in the population were to double from 40 to 80. Then, with a sample size of 1000, the probability that $\widehat{C} = 0$ would fall from `r round(100 * d_C_hat(0, 1000, 40))`\% to `r round(100 * d_C_hat(0, 1000, 80))`\%. As a rough guide, a sample size between one and two thousand tests per week should be sufficiently informative to detect cases when the prevalence is 0.2\% or above. 

```{r, echo = FALSE, fig.height = 4}
plot_C_hat(1000, 80)
```



## Pooled Testing

## What about false positives and negatives?

[^1]: The exact value is 20,001/$(C + 1)$.

